%!TEX root = ../report.tex

\begin{document}
	\chapter{State of the art analysis} \label{stateof}
	
	\section{Databases for Robotic applications}
		\citet{article02} integrated a well known NoSQL database Cassandra with the Robotic Operating System(ROS) to handle the data in smart environments. Evaluated this approach in two scenarios, within a realistic robot exploration, and with randomly generated data. The final solution is compared with two other solutions that are commonly used in ROS applications(rosbag and mongo\_ros). The research results show that Cassandra can handle terabytes of data and their time-stamp mechanism allows querying and retrieving data without additional efforts. But this approach compared their results only with two other storage mechanisms. 
		
		\citet{article04} discussed the importance of memory recall mechanism for robots briefly. Also, implemented a two-level database layer in the centralized server, to store simple data(odometric values) in a graph database (Neo4j) and more considerable sensor data like RGB and depth image and laser scans in key-value store (MongoDB). With this architecture, they can achieve the sharing of workload between robot and database server. As a result, for executing this query server took between 10 to 100 milliseconds. In this work, they have given an initial motivation to the robotics community that using databases is possible and advisable, but the author didn't specify why  Neo4j and MongoDB for data storing even though there are a lot of database technologies available.
		
		\citet{article03} briefly explains the difficulties in handling the log messages from ROS(Robotic Operating System) and alternate ideas to store ROS logs for utilizing it in the future. To overcome the challenges in storing logs in a flat file system, they wanted to use the relational database or NoSQL database. So a benchmarking test has been conducted to evaluate and find which one is best. Three databases (MongoDB, PostgreSQL, SQLite3) from NoSQL and relational database family has been chosen for benchmarking. But in recent years, PostgreSQL and SQLite3 databases are little outdated, and MongoDB is not only the NoSQL database available. New NoSQL databases are emerging every month and might perform better than MongoDB. So these evaluation results are not useful to choose the best NoSQL database to store sensors logs and benchmarking only one NoSQL database is not an optimal choice.
		
		From all these approaches discussed above, they focused on particular applications/settings and are always comparing only very few databases within this limited setting. Moreover, they benchmarked the performance with a single robot in most of the cases. Therefore, in our previous work \cite{ravichandranworkbench}, we have conducted a qualitative and quantitative analysis of varieties of databases to find the best performing databases for multi-robot applications. But the results show that the performance differs for different scenarios and type of sensor data. Hence to support multi-database in robotic applications, we chose to create a mediator component with generalized semantic data models that help to solve querying heterogeneous data in a meaningful way. In the next section, we review types of existing semantic data models and mediator architectures that support this research work.
		
	\section{Semantic Data models for sensors}
	\citet{su2014connecting} highlights the interoperability issues in IoT sensor data and also says that these data should be useful for multiple applications rather than dependent on specific domain. To make the machines interpret the meaning of sensor data, author suggested to use Semantic Web technologies such as Resource Description Framework (RDF). Even though SenML is an evolving technique to model to sensor data, but it is lacking reasoning capabilities and interoperability with other devices. To overcome the issues in SenML generic model, author proposes a technique to represent IoT sensors as a Knowledge Based Systems by transforming SenML to Resource Description Framework. 
	
	As an advantage, the transformed data can be analyzed and helpful to take more meaningful actions. 
	SenML is specially designed for resource constrained devices, hence additional information to contextually understand the data has not been included intentionally. Each entry of SenML data should have the sensor parameter name and other attributes such as time, value, etc. Also it supports custom attribute called Resource Type (rt) to let the users add their own attributes and this allow users to include contextual information.
	
	With the help of transformation we could adopt RDF structure to model robot generated sensor data, But \citet{charpenay2018towards} points out that RDF data structure is not suitable for resource constrained devices like micro-controllers and also analyzed the issues in RDF such as verbosity and complexity in processing knowledge. To overcome this issues, author carried out an extensive analysis between RDF and JSON-LD structures. JSON-LD was published by W3C, and it serves as an alternative for RDF. Using JSON-LD one can represent the context for the data which is more important in robotics field such that other robots can understand the data based on context. Results shows that JSON-LD compaction coupled with EXI4-JSON or CBOR outperforms state-of-the-art (HDT) with \textbf{50 - 60 \% }compaction ratios. This is one of the reason why we chose JSON-LD to represent the context in our data models.
	
	\section{Mediator architectures}
    In this section, different approaches for mediator architectures and mechanisms for querying heterogeneous data are presented to grasp the core ideas and how could we optimize the existing mediator models.
	
	\citet{fahl1993amos} proposed an active mediator architecture to gather information from different knowledge base and combine them to a single response. AMOS\footnote{\label{amos}Active Mediators Object System} architecture uses Object-Oriented approach to define declarative queries. This distributed architecture involves multiple mediator modules to work collaboratively to collect the required piece of information and produce final result. Primary components of AMOS architecture are,
	\begin{itemize}
		\item Integrator - Gather data from multiple data sources that have different data representations.
		\item Monitor - Monitor service always watch for any data changes and notifies the mediators. This is helpful in the case where system needs an active updates to change its current task.
		\item Domain models represents the models related to application which helps to access data easier from any database through a query language.
		\item Locators helps to locate mediators in the network.
	\end{itemize}
	
	
	Integrator module is built with two internal components called IAMOS\footnote{\label{amos}Integration Active Mediators Object System} and TAMOS\footnote{\label{amos}Translation Active Mediators Object System}. 
	First "Integration AMOS (IAMOS)" parse the query and send individual requests to Translational AMOS modules which are responsible for heterogeneous data source.
	Then, all TAMOS modules return the individual results to IAMOS for integrating all the results. To query multi databases from IAMOS, IAMOS servers are mapped with TAMOS servers with the help of Object-Oriented query language.
	
	\citet{ahmed1991pegasus} developed a system called Pegasus that supports multiple heterogeneous database systems with various data bases models, query languages and services. Pegasus predefines its domain data models based on object oriented approach and also supports programming capabilities. These objects are created and mapped with the types and functions with the help of HOSQL\footnote{\label{myfootnote}Heterogeneous Object Structured Query Language} statements. HOSQL is a declarative object oriented query language which is used by Pegasus to manipulate data from multiple data sources.
	
	
	
	Pegasus system supports two types of data sources, local and native data sources. Whenever a new data source joins Pegasus system, schema integrator module imports schema from data source and update its root schema with the new schema types. The final integrated schema shows the complete blueprint of the different data sources participates in the data integration. Pegasus system work-flow is comparatively similar to AMOS architecture, but they use different query language and data modeling strategies.
	
	\citet{chawathe1994tsimmis} developed project Tsimmiss extract information from any kind of data source and translates them to a meaningful common object. Unlike AMOS and Pegasus, Tsimmiss follows a straight forward approach to define the data model which is a self-describing object model. Each object must contain a label, type and value itself. Label can be used by the system to understand the meaning of the value and type shows the observed value type. Objects can be nested together to form a set of objects. 
	
	Tsimmiss tool offers a unique query language called OEM-QL and this language follows the SQL query language pattern to fetch the data from mediators. Mediators resolves the query and send separate requests to respective data sources to retrieve the information and merge them together to give a single response back to user.
	
	In the articles discussed above, mediators are targeted to extract information from different data sources that could be different databases or data from file-system. But Rufus system proposed by \citet{shoens1993rufus} focus only on semi structured data stored in file system for example documents, objects, programming files, mail, binary files, images etc. Rufus system classifier automatically classifies the type of file and apply a scanning mechanism on those files to extract the required information and transform them to the appropriate data model which is understandable by Rufus system. Rufus can classify 34 different classes of files.
	
	\citet{papakonstantinou1996medmaker} proposes a Mediator Specification Language (MSL) that helps the mediator to understand the schema and integrates the data from unstructured or semistructured source. MSL overcomes the major problems in existing mediator systems for example,
	\begin{itemize}
		\item \emph{Schema domain mismatch} - Attribute name mismatch or attribute is fragmented to sub-attributes between different domains. For example, domain A uses attribute "fullname" that consists of both first and last name, and domain B uses two attributes "first\_name" and "last\_name" to store users full name.
		\item \emph{Schematic discrepancy} - Storing data between tables inconsistently. For example, in "domain A" users data is stored in the "user" table, and their status is stored in "status" table, and in "domain B" the status is stored in "user" table itself.
		\item \emph{Schematic evaluation} - Data schema and attributes may change over time without notifying mediator system. For example, the "age" attribute is added in one domain or data type of "price" is altered from Integer to Float.
		\item \emph{Structure irregularities} - Dissimilarity between data schemas from different domains.
	\end{itemize}
	
	During translation of original information from different sources to a single object it should be important that, all data sources should have the required attribute and the name of the attribute should be same. Otherwise, mediator system will not be able to process the information to a single answer.  External predicates and Creation of the Virtual Objects in MSL solves the problems mentioned above.
	
	\citet{arens1996query} built a mediator which is flexible to map domain level query different data-sources and efficient to plan the query execution to reduce the overall execution time. Information source models provides relations between the super class and subclasses, and also the mapping between the domain models and information from heterogeneous sources. SIMS uses Loom as a representational language to make objects and relationship between them. SIMS supports parallel query access plan that makes the mediator to access information independent of data sources and the user will get the final answer as quick as possible.
	
	In summary, all the architectures reviewed above focus on one significant activity of mapping the heterogeneous data to a single representation. However, each architecture proposes its own way of representing data and a tool to query the federation data. This way imposes more load on the developers to translate their existing data schema into the mediator required representation and learn a new query language. We overcome this issue with the help of our database schema translator that automatically translates the schema of the existing data model to the mediator required format. Also,  using views to abstract data schema is one possible approach to solve heterogeneity. However, in our strategy, we tried to add on-demand contexts using JSON-LD for each entity in the robot ecosystem to make the data consumable by other robots/tools. In the next section, we describe why context is important to identify the attributes in a data semantically. Finally, none of the architectures considered declarative data fetching, but in this research work, we give attention to declarative data fetching frameworks which improves the flexibility for the users to fetch only required attributes. It will also be useful when robots want to save data and reduce latency in poor network connectivity areas.
	
	\section{Declarative data fetching frameworks}
	
	\citet{cederlund2016performance} performed an extensive comparison between REST, GraphQL and Falcor by declarative data fetching. They evaluated all three frameworks based on latency, data volume, and many requests with real-world test cases. Also, they analyzed the efficiency of filtering done by the frameworks.
	
	Their results reveal that Relay+GraphQL decreases the response time under parallel and sequential data flow. Furthermore, the response size is decreased when using the frameworks rather than REST API's. However, within the frameworks, Falcor response time and size is high compared to GraphQL. Ultimately, both the frameworks reduced the number of network calls to a single request.
	
	As a conclusion, \citet{cederlund2016performance} suggests to use custom REST endpoints since the frameworks increase the size of requests, but it still depends on the application requirement. In our application, Robots might work in the places where limited network access available, so we definitely use these frameworks as a base on our mediator to reduce the response size. Moreover, the observation data may have too many information which are unnecessary for the other robots or tools to work on. Therefore only picking the necessary fields in the response profoundly reduce the response size as well as the response time.

	
	Many mediator systems have been developed in the past to support integrating heterogeneous information from different data sources. All of them built with different architectures, query language, and execution optimization. In our mediator approach we focus mainly on,
	\begin{itemize}
		\item How different type of robot generated data will be stored in multiple data sources?
		\item A unique context based data model to represent the components attached with each robot and data generated by them.
		\item Semantic query language to communicate with mediator. Unlike traditional query languages we would like to attempt new way of querying data, for example Graphql.
		\item GUI tool to visualize and analyze the robot generated data in a meaningful way.
	\end{itemize} 

\end{document}
